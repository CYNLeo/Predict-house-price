# -*- coding: utf-8 -*-
"""21073251A_ChongYeeNam_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cRDIbogyf4MrAMlkGMQzCgNnd9l5HMvB
"""

def showinfo():
  print('Name : Chong Yee Nam')
  print('Student ID: 21073251A')
  print('Class : 201B')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
df = pd.read_csv('/content/drive/MyDrive/Data_ams2/Q1_Housingv1.csv')

features_with_na=[features for features in df.columns if df[features].isnull().sum()>1]
for feature in features_with_na:
    print(feature, np.round(df[feature].isnull().mean(), 4),  ' % missing values')

"""We can see that 'MiscFeature','Fence','PoolQC','Alley' and 'FireplaceQu' has high precent of missing values, therefore we drop these columns"""

df['Utilities'].unique()

"""We can see that 'Utilities' column only has one value therefore we drop this column"""

df.drop(['MiscFeature','Fence','PoolQC','Alley','Utilities','FireplaceQu'],axis = 1,inplace=True) #dropping Utilities because all the value are same

#chnage to string (Date value or identify the type,etc)
df.dropna(inplace=True)
df['GarageYrBlt'] = df['GarageYrBlt'].astype(int)
df['MSSubClass'] =df['MSSubClass'].astype(str)
df['OverallQual'] =df['OverallQual'].astype(str)
df['OverallCond'] =df['OverallCond'].astype(str)
df['YrSold'] =df['YrSold'].astype(str)
df['YearRemodAdd'] = df['YearRemodAdd'].astype(str)
df['YearBuilt'] = df['YearBuilt'].astype(str)
df['GarageYrBlt'] = df['GarageYrBlt'].astype(str)

"""The above columns cahnge to string value becasue some of them is belong to categoical columns such as year or date"""

from sklearn.preprocessing import LabelEncoder

num_cols = ['LotFrontage', 'LotArea','MoSold',
            'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',
            'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',
            'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',
            'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',
            'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF','EnclosedPorch',
            '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']

temp= [
  "MSSubClass",
  "MSZoning",
  "Street",
  "LotShape",
  "LandContour",
  "LotConfig",
  "LandSlope",
  "Neighborhood",
  "Condition1",
  "Condition2",
  "BldgType",
  "HouseStyle",
  "YearBuilt",
  "YearRemodAdd",
  "RoofStyle",
  "RoofMatl",
  "Exterior1st",
  "Exterior2nd",
  "MasVnrType",
  "ExterQual",
  "ExterCond",
  "Foundation",
  "BsmtQual",
  "BsmtCond",
  "BsmtExposure",
  "BsmtFinType1",
  "BsmtFinType2",
  "Heating",
  "HeatingQC",
  "CentralAir",
  "Electrical",
  "KitchenQual",
  "Functional",
  "GarageType",
  "GarageYrBlt",
  "GarageFinish",
  "GarageQual",
  "GarageCond",
  "PavedDrive",
  "YrSold",
  "SaleType",
  "SaleCondition"
]

for c in temp:
  Ibl = LabelEncoder()
  Ibl.fit(list(df[c].values))
  df[c] = Ibl.transform(list(df[c].values))

df.head()

df.info()

from sklearn.preprocessing import MinMaxScaler, StandardScaler
scaler = MinMaxScaler()
std = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])
df['SalePrice'] = scaler.fit_transform(df['SalePrice'].values.reshape(-1,1))

X = df.drop('SalePrice',axis = 1)
y = df['SalePrice']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)
x_train.drop('Id',axis = 1,inplace = True)
test_id = x_test['Id']
x_test.drop('Id',axis = 1,inplace = True)

# Importing LinearRegression
from sklearn.linear_model import LinearRegression
#Training Linear regression model
lr = LinearRegression()
lr.fit(x_train, y_train)
LR_y_pred = lr.predict(x_test)
expl_lr = explained_variance_score(LR_y_pred,y_test)
lr_score = lr.score(x_test,y_test)
mae_lr = metrics.mean_absolute_error(y_test,LR_y_pred)
mse_lr =  metrics.mean_squared_error(y_test,LR_y_pred)

from sklearn import metrics
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import explained_variance_score
from sklearn.metrics import confusion_matrix

#Training Decision Tree model
tr_regressor = DecisionTreeRegressor(random_state=0)
tr_regressor.fit(x_train,y_train)
tr_regressor.score(x_test,y_test)
pred_tr = tr_regressor.predict(x_test)
decision_score=tr_regressor.score(x_test,y_test)
expl_tr = explained_variance_score(pred_tr,y_test)
mae_tr = metrics.mean_absolute_error(y_test,pred_tr)
mse_tr = metrics.mean_squared_error(y_test,pred_tr)

#Training Random Forest model
rf_regressor = RandomForestRegressor(n_estimators=28,random_state=0)
rf_regressor.fit(x_train,y_train)
rf_regressor.score(x_test,y_test)
rf_pred =rf_regressor.predict(x_test)
rf_score=rf_regressor.score(x_test,y_test)
expl_rf = explained_variance_score(rf_pred,y_test)
mae_rf = metrics.mean_absolute_error(y_test,rf_pred)
mse_rf = metrics.mean_squared_error(y_test,rf_pred)

#Training SVM model
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn import metrics
cls = svm.SVR(kernel='linear')
cls.fit(x_train, y_train)
pred = cls.predict(x_test)
svm_score = cls.score(x_test,y_test)
mae_svm = metrics.mean_absolute_error(y_test,pred)
mse_svm = metrics.mean_squared_error(y_test,pred)
expl_svm = explained_variance_score(pred,y_test)

"""**Linear Regression**"""

print(r2_score(y_test,LR_y_pred))
print('MAE:', metrics.mean_absolute_error(y_test,LR_y_pred))
print('MSE:', metrics.mean_squared_error(y_test,LR_y_pred))
print('RMSE:',np.sqrt(metrics.mean_absolute_error(y_test,LR_y_pred)))

"""**Random Forest and Decision Tree**"""

print("Decision tree Regression Model Score is",round(tr_regressor.score(x_test,y_test)*100))
print("Random Forest Regression Model Score is",round(rf_regressor.score(x_test,y_test)*100))

"""**SVM**"""

print(r2_score(y_test,pred))
print('MAE:', metrics.mean_absolute_error(y_test,pred))
print('MSE:', metrics.mean_squared_error(y_test,pred))
print('RMSE:',np.sqrt(metrics.mean_absolute_error(y_test,pred)))

models_score = pd.DataFrame({'Model': ['Decision Tree','Random forest ', 'SVM', 'Linear Regression'],
'R^2 Score': [decision_score, rf_score, svm_score, lr_score],
'Explained Variance Score': [expl_tr, expl_rf, expl_svm, expl_lr],
'Mean Absolute Error': [mae_tr, mae_rf, mae_svm, mae_lr],
'Mean Squared Error': [mse_tr, mse_rf, mse_svm, mse_lr] })
models_score.sort_values(by='R^2 Score', ascending=False)

"""**Conclusion**

According to the score with different model,since Linear Regression has a high R2 score which are very close to 100, it suspicious to overfitting. Therefore, I will use the second high model (Random forest) to train the data
"""

temp = pd.DataFrame({'SalePrice': rf_pred.flatten()})

temp_price = scaler.inverse_transform(temp)

temp_price = temp_price.ravel()

submission = pd.DataFrame({'Id': test_id, 'SalePrice': temp_price})

submission = submission.reset_index()
submission.drop('index',axis = 1,inplace = True)

showinfo()

submission